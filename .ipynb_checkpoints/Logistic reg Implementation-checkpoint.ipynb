{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re, collections\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "#Advanced optimization\n",
    "from scipy import optimize as op\n",
    "import seaborn as sb\n",
    "from sklearn.metrics import confusion_matrix #Confusion matrix\n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from pandas.tools.plotting import parallel_coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('essays_and_scores.csv', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  domain1_score  \n",
       "0             4.0             4.0            8.0  \n",
       "1             5.0             4.0            9.0  \n",
       "2             4.0             3.0            7.0  \n",
       "3             5.0             5.0           10.0  \n",
       "4             4.0             4.0            8.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       essay_set                                              essay  \\\n",
      "0              1  Dear local newspaper, I think effects computer...   \n",
      "1              1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2              1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3              1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4              1  Dear @LOCATION1, I know having computers has a...   \n",
      "5              1  Dear @LOCATION1, I think that computers have a...   \n",
      "6              1  Did you know that more and more people these d...   \n",
      "7              1  @PERCENT1 of people agree that computers make ...   \n",
      "8              1  Dear reader, @ORGANIZATION1 has had a dramatic...   \n",
      "9              1  In the @LOCATION1 we have the technology of a ...   \n",
      "10             1  Dear @LOCATION1, @CAPS1 people acknowledge the...   \n",
      "11             1  Dear @CAPS1 @CAPS2 I feel that computers do ta...   \n",
      "12             1  Dear local newspaper I raed ur argument on the...   \n",
      "13             1  My three detaileds for this news paper article...   \n",
      "14             1  Dear, In this world today we should have every...   \n",
      "15             1  Dear @ORGANIZATION1, The computer blinked to l...   \n",
      "16             1  Dear Local Newspaper, I belive that computers ...   \n",
      "17             1  Dear Local Newspaper, I must admit that the ex...   \n",
      "18             1  I aegre waf the evansmant ov tnachnolage. The ...   \n",
      "19             1  Well computers can be a good or a bad thing. I...   \n",
      "20             1  Dear @CAPS1 of the @CAPS2 @CAPS3 daily, I am w...   \n",
      "21             1  Dear local Newspaper @CAPS1 a take all your co...   \n",
      "22             1  Dear local newspaper, @CAPS1 you ever see a ch...   \n",
      "23             1  Dear local newspaper, I've heard that not many...   \n",
      "24             1  Dear @CAPS1, @CAPS2 off, I beileve that comput...   \n",
      "25             1  Do you think that computers are useless? Or do...   \n",
      "26             1  Computers a good because you can get infermati...   \n",
      "27             1  Dear Newspaper, Computers are high tec and hav...   \n",
      "28             1  Dear local newspaper, @CAPS1 people throughout...   \n",
      "29             1  Dear Newspaper People, I think that computers ...   \n",
      "...          ...                                                ...   \n",
      "12948          8   We all understand the benefits of laughter. L...   \n",
      "12949          8        It was midsummer, and i could feel the c...   \n",
      "12950          8   Have you ever experienced a time with your fr...   \n",
      "12951          8   I woke up just like any other day happy yet l...   \n",
      "12952          8   Laughter is an important part of my life, eit...   \n",
      "12953          8   I sat at the table, speechless, as they told ...   \n",
      "12954          8   As I remember back, it was @DATE1. It was a h...   \n",
      "12955          8   Those eyes, it was like I was looking out int...   \n",
      "12956          8  Some say that laugh is the common language bet...   \n",
      "12957          8   Laughter is an integral element to many situa...   \n",
      "12958          8  One time I was at my friend @PERSON1's house, ...   \n",
      "12959          8   LAUGHTER @CAPS1 knows that laughter is a heal...   \n",
      "12960          8  One thing that people in the world love to do ...   \n",
      "12961          8   Laughter, to me, is an important aspect of my...   \n",
      "12962          8   People always say that the worst parts of lif...   \n",
      "12963          8   Why is it that people can look back at someth...   \n",
      "12964          8   Before my best friend moved away, we would st...   \n",
      "12965          8                                @ORGANIZATION1  ...   \n",
      "12966          8   Morose and somnolent, I woke up. I woke up to...   \n",
      "12967          8   A while back my mom had decided to send me to...   \n",
      "12968          8                              I dont like computers   \n",
      "12969          8   Everyone knows how important a laugh can be. ...   \n",
      "12970          8   Laughter is an important part of my family. W...   \n",
      "12971          8   laughter is an important part of any kind of ...   \n",
      "12972          8  Sometime ago on a hot @DATE1 day my @NUM1 ,@PE...   \n",
      "12973          8   In most stories mothers and daughters are eit...   \n",
      "12974          8   I never understood the meaning laughter is th...   \n",
      "12975          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
      "12976          8                                 Trippin' on fen...   \n",
      "12977          8   Many people believe that laughter can improve...   \n",
      "\n",
      "       domain1_score  \n",
      "0                8.0  \n",
      "1                9.0  \n",
      "2                7.0  \n",
      "3               10.0  \n",
      "4                8.0  \n",
      "5                8.0  \n",
      "6               10.0  \n",
      "7               10.0  \n",
      "8                9.0  \n",
      "9                9.0  \n",
      "10               8.0  \n",
      "11               8.0  \n",
      "12               7.0  \n",
      "13               6.0  \n",
      "14               6.0  \n",
      "15              12.0  \n",
      "16               8.0  \n",
      "17               8.0  \n",
      "18               4.0  \n",
      "19               6.0  \n",
      "20               8.0  \n",
      "21               3.0  \n",
      "22              10.0  \n",
      "23              11.0  \n",
      "24               8.0  \n",
      "25               9.0  \n",
      "26               4.0  \n",
      "27               9.0  \n",
      "28               9.0  \n",
      "29               8.0  \n",
      "...              ...  \n",
      "12948           40.0  \n",
      "12949           32.0  \n",
      "12950           36.0  \n",
      "12951           31.0  \n",
      "12952           30.0  \n",
      "12953           47.0  \n",
      "12954           40.0  \n",
      "12955           35.0  \n",
      "12956           33.0  \n",
      "12957           36.0  \n",
      "12958           36.0  \n",
      "12959           48.0  \n",
      "12960           40.0  \n",
      "12961           40.0  \n",
      "12962           40.0  \n",
      "12963           42.0  \n",
      "12964           40.0  \n",
      "12965           32.0  \n",
      "12966           36.0  \n",
      "12967           40.0  \n",
      "12968           10.0  \n",
      "12969           33.0  \n",
      "12970           44.0  \n",
      "12971           35.0  \n",
      "12972           30.0  \n",
      "12973           35.0  \n",
      "12974           32.0  \n",
      "12975           40.0  \n",
      "12976           40.0  \n",
      "12977           40.0  \n",
      "\n",
      "[12978 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# getting relevant columns\n",
    "\n",
    "data = dataframe[['essay_set','essay','domain1_score']].copy()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence into words\n",
    "\n",
    "def sentence_to_wordlist(raw_sentence):\n",
    "    \n",
    "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "    tokens = nltk.word_tokenize(clean_sentence)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing an essay into a list of word lists\n",
    "\n",
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating average word length in an essay\n",
    "\n",
    "def avg_word_len(essay):\n",
    "    \n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    \n",
    "    return sum(len(word) for word in words) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating number of words in an essay\n",
    "\n",
    "def word_count(essay):\n",
    "    \n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(clean_essay)\n",
    "    \n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating number of characters in an essay\n",
    "\n",
    "def char_count(essay):\n",
    "    \n",
    "    clean_essay = re.sub(r'\\s', '', str(essay).lower())\n",
    "    \n",
    "    return len(clean_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating number of sentences in an essay\n",
    "\n",
    "def sent_count(essay):\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    \n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating number of lemmas per essay\n",
    "\n",
    "def count_lemmas(essay):\n",
    "    \n",
    "    tokenized_sentences = tokenize(essay)      \n",
    "    \n",
    "    lemmas = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence) \n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "        \n",
    "            pos_tag = token_tuple[1]\n",
    "        \n",
    "            if pos_tag.startswith('N'): \n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('J'):\n",
    "                pos = wordnet.ADJ\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('V'):\n",
    "                pos = wordnet.VERB\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('R'):\n",
    "                pos = wordnet.ADV\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            else:\n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "    \n",
    "    lemma_count = len(set(lemmas))\n",
    "    \n",
    "    return lemma_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking number of misspelled words\n",
    "\n",
    "def count_spell_error(essay):\n",
    "    \n",
    "    clean_essay = re.sub(r'\\W', ' ', str(essay).lower())\n",
    "    clean_essay = re.sub(r'[0-9]', '', clean_essay)\n",
    "    \n",
    "    #big.txt: It is a concatenation of public domain book excerpts from Project Gutenberg \n",
    "    #         and lists of most frequent words from Wiktionary and the British National Corpus.\n",
    "    #         It contains about a million words.\n",
    "    data = open('big.txt').read()\n",
    "    \n",
    "    words_ = re.findall('[a-z]+', data.lower())\n",
    "    \n",
    "    word_dict = collections.defaultdict(lambda: 0)\n",
    "                       \n",
    "    for word in words_:\n",
    "        word_dict[word] += 1\n",
    "                       \n",
    "    clean_essay = re.sub(r'\\W', ' ', str(essay).lower())\n",
    "    clean_essay = re.sub(r'[0-9]', '', clean_essay)\n",
    "                        \n",
    "    mispell_count = 0\n",
    "    \n",
    "    words = clean_essay.split()\n",
    "                        \n",
    "    for word in words:\n",
    "        if not word in word_dict:\n",
    "            mispell_count += 1\n",
    "    \n",
    "    return mispell_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating number of nouns, adjectives, verbs and adverbs in an essay\n",
    "\n",
    "def count_pos(essay):\n",
    "    \n",
    "    tokenized_sentences = tokenize(essay)\n",
    "    \n",
    "    noun_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    adv_count = 0\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence)\n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "        \n",
    "            if pos_tag.startswith('N'): \n",
    "                noun_count += 1\n",
    "            elif pos_tag.startswith('J'):\n",
    "                adj_count += 1\n",
    "            elif pos_tag.startswith('V'):\n",
    "                verb_count += 1\n",
    "            elif pos_tag.startswith('R'):\n",
    "                adv_count += 1\n",
    "            \n",
    "    return noun_count, adj_count, verb_count, adv_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getiing Bag of Words (BOW) counts\n",
    "\n",
    "def get_count_vectors(essays):\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features = 10000, ngram_range=(1, 3), stop_words='english')\n",
    "    \n",
    "    count_vectors = vectorizer.fit_transform(essays)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    return feature_names, count_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting essay features\n",
    "\n",
    "def extract_features(data):\n",
    "    \n",
    "    features = data.copy()\n",
    "    \n",
    "    features['char_count'] = features['essay'].apply(char_count)\n",
    "    \n",
    "    features['word_count'] = features['essay'].apply(word_count)\n",
    "    \n",
    "    features['sent_count'] = features['essay'].apply(sent_count)\n",
    "    \n",
    "    features['avg_word_len'] = features['essay'].apply(avg_word_len)\n",
    "    \n",
    "    features['lemma_count'] = features['essay'].apply(count_lemmas)\n",
    "    \n",
    "    features['spell_err_count'] = features['essay'].apply(count_spell_error)\n",
    "    \n",
    "    features['noun_count'], features['adj_count'], features['verb_count'], features['adv_count'] = zip(*features['essay'].map(count_pos))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      essay_set                                              essay  \\\n",
      "0             1  Dear local newspaper, I think effects computer...   \n",
      "1             1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2             1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3             1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4             1  Dear @LOCATION1, I know having computers has a...   \n",
      "5             1  Dear @LOCATION1, I think that computers have a...   \n",
      "6             1  Did you know that more and more people these d...   \n",
      "7             1  @PERCENT1 of people agree that computers make ...   \n",
      "8             1  Dear reader, @ORGANIZATION1 has had a dramatic...   \n",
      "9             1  In the @LOCATION1 we have the technology of a ...   \n",
      "10            1  Dear @LOCATION1, @CAPS1 people acknowledge the...   \n",
      "11            1  Dear @CAPS1 @CAPS2 I feel that computers do ta...   \n",
      "12            1  Dear local newspaper I raed ur argument on the...   \n",
      "13            1  My three detaileds for this news paper article...   \n",
      "14            1  Dear, In this world today we should have every...   \n",
      "15            1  Dear @ORGANIZATION1, The computer blinked to l...   \n",
      "16            1  Dear Local Newspaper, I belive that computers ...   \n",
      "17            1  Dear Local Newspaper, I must admit that the ex...   \n",
      "18            1  I aegre waf the evansmant ov tnachnolage. The ...   \n",
      "19            1  Well computers can be a good or a bad thing. I...   \n",
      "20            1  Dear @CAPS1 of the @CAPS2 @CAPS3 daily, I am w...   \n",
      "21            1  Dear local Newspaper @CAPS1 a take all your co...   \n",
      "22            1  Dear local newspaper, @CAPS1 you ever see a ch...   \n",
      "23            1  Dear local newspaper, I've heard that not many...   \n",
      "24            1  Dear @CAPS1, @CAPS2 off, I beileve that comput...   \n",
      "25            1  Do you think that computers are useless? Or do...   \n",
      "26            1  Computers a good because you can get infermati...   \n",
      "27            1  Dear Newspaper, Computers are high tec and hav...   \n",
      "28            1  Dear local newspaper, @CAPS1 people throughout...   \n",
      "29            1  Dear Newspaper People, I think that computers ...   \n",
      "...         ...                                                ...   \n",
      "1753          1  Dear local newspaper, @CAPS1 on a beautiful su...   \n",
      "1754          1  Dear @CAPS1, I believe that computers have a n...   \n",
      "1755          1  I think we can all agree that computer usage i...   \n",
      "1756          1  Dear @PERSON1, Computers are very helpful in d...   \n",
      "1757          1  Dear Newspaper, @CAPS1 are worried that people...   \n",
      "1758          1  Dear Local Newspaper: @CAPS1 you know that ove...   \n",
      "1759          1  Dear @PERSON1, The advansing technology is sho...   \n",
      "1760          1  Dear local Newspaper I ting that computers are...   \n",
      "1761          1  Man has always been interested in technology. ...   \n",
      "1762          1  Guaranteed, @NUM1 years from now we will still...   \n",
      "1763          1  I think the effects of the computer are bad, t...   \n",
      "1764          1  Dear editor, I think people are using computer...   \n",
      "1765          1  Dear @CAPS1 @CAPS2, @CAPS3, experts have been ...   \n",
      "1766          1  Computers, a @LOCATION1 topic if you ask me. S...   \n",
      "1767          1  Dear Newspaper Readers, @CAPS1 many hours a da...   \n",
      "1768          1  Dear @CAPS1 newspaper, I have resently read th...   \n",
      "1769          1  Dear @ORGANIZATION2 (our local newspaper), @CA...   \n",
      "1770          1  Dear newspaper, In my opinion computers do ben...   \n",
      "1771          1  Technology, such as computers are very big. I ...   \n",
      "1772          1  Dear Newspaper, Computers have advance a lot s...   \n",
      "1773          1  Dear Newspaper, I think that computers have a ...   \n",
      "1774          1  Dear @LOCATION1, *@CAPS1*. Now I hear my favor...   \n",
      "1775          1  Dear Newspaper I think that computers were one...   \n",
      "1776          1  Mom!!! Did you know that the human body has on...   \n",
      "1777          1  Dear @ORGANIZATION1, I believe that computers ...   \n",
      "1778          1  Dear @CAPS1, @CAPS2 several reasons on way I t...   \n",
      "1779          1  Do a adults and kids spend to much time on the...   \n",
      "1780          1  My opinion is that people should have computer...   \n",
      "1781          1  Dear readers, I think that its good and bad to...   \n",
      "1782          1  Dear - Local Newspaper I agree thats computers...   \n",
      "\n",
      "      domain1_score  char_count  word_count  sent_count  avg_word_len  \\\n",
      "0               8.0        1538         350          16      4.237143   \n",
      "1               9.0        1870         423          20      4.312057   \n",
      "2               7.0        1263         283          14      4.342756   \n",
      "3              10.0        2642         530          27      4.813208   \n",
      "4               8.0        2105         473          30      4.334038   \n",
      "5               8.0        1031         247          15      4.052632   \n",
      "6              10.0        2310         508          30      4.385827   \n",
      "7              10.0        2243         508          39      4.242126   \n",
      "8               9.0        1960         451          35      4.190687   \n",
      "9               9.0        2131         519          26      3.982659   \n",
      "10              8.0        1639         330          22      4.812121   \n",
      "11              8.0        1791         400          25      4.375000   \n",
      "12              7.0         804         204           6      3.906863   \n",
      "13              6.0        1311         307          25      4.188925   \n",
      "14              6.0         816         177          13      4.485876   \n",
      "15             12.0        2641         534          35      4.799625   \n",
      "16              8.0        1470         347          18      4.092219   \n",
      "17              8.0        1594         374          15      4.125668   \n",
      "18              4.0         297          66           7      4.409091   \n",
      "19              6.0         713         160          11      4.300000   \n",
      "20              8.0        1683         368          20      4.467391   \n",
      "21              3.0         262          56           2      4.625000   \n",
      "22             10.0        2358         530          30      4.313208   \n",
      "23             11.0        2658         576          39      4.397569   \n",
      "24              8.0        1381         296          16      4.550676   \n",
      "25              9.0        1645         363          22      4.338843   \n",
      "26              4.0         531         122           7      4.188525   \n",
      "27              9.0        1687         363          28      4.515152   \n",
      "28              9.0        1692         378          23      4.306878   \n",
      "29              8.0        1165         264          15      4.299242   \n",
      "...             ...         ...         ...         ...           ...   \n",
      "1753           10.0        2460         524          38      4.503817   \n",
      "1754           10.0        1563         330          23      4.612121   \n",
      "1755           12.0        2508         566          45      4.247350   \n",
      "1756            8.0        1575         358          21      4.229050   \n",
      "1757            8.0        1449         343          17      4.110787   \n",
      "1758           12.0        2524         476          33      5.025210   \n",
      "1759            8.0        1818         408          24      4.299020   \n",
      "1760            5.0         443         100           6      4.320000   \n",
      "1761            8.0        1678         370          30      4.381081   \n",
      "1762            9.0        1912         419          25      4.427208   \n",
      "1763            8.0        2079         475          12      4.273684   \n",
      "1764            9.0        1371         286          20      4.650350   \n",
      "1765           10.0        1719         350          22      4.697143   \n",
      "1766            8.0        1640         374          26      4.219251   \n",
      "1767           10.0        2598         509          33      4.856582   \n",
      "1768            8.0        1399         327          18      4.122324   \n",
      "1769           10.0        2040         411          23      4.771290   \n",
      "1770            9.0        1663         369          18      4.387534   \n",
      "1771            9.0        1713         396          19      4.219697   \n",
      "1772           11.0        2625         616          30      4.121753   \n",
      "1773            5.0         517         116           4      4.396552   \n",
      "1774           10.0        1982         448          31      4.169643   \n",
      "1775            9.0        2315         466          27      4.766094   \n",
      "1776           10.0        1946         474          24      3.951477   \n",
      "1777            8.0        1521         333          23      4.432432   \n",
      "1778            8.0        2094         509          21      4.015717   \n",
      "1779            7.0         892         213          18      4.028169   \n",
      "1780            8.0        1352         296          18      4.489865   \n",
      "1781            2.0          57          15           1      3.733333   \n",
      "1782            7.0         905         216          18      4.092593   \n",
      "\n",
      "      lemma_count  spell_err_count  noun_count  adj_count  verb_count  \\\n",
      "0             162               11          83         18          74   \n",
      "1             185               25         107         19          85   \n",
      "2             145                5          82         20          52   \n",
      "3             236               34         178         42          97   \n",
      "4             190               19         114         32          90   \n",
      "5             127               15          53         12          49   \n",
      "6             213               10         136         31          80   \n",
      "7             202               14         133         32         112   \n",
      "8             214               11         110         32          86   \n",
      "9             204               24         109         40         112   \n",
      "10            203               14          88         31          70   \n",
      "11            154               36         102         39          84   \n",
      "12            112               24          43         17          43   \n",
      "13            127               40          81         35          57   \n",
      "14             97               16          57         15          30   \n",
      "15            246               35         168         39         103   \n",
      "16            138               11          71         19          74   \n",
      "17            166               10          79         24          77   \n",
      "18             50               33          25          5          14   \n",
      "19             92                8          52         14          27   \n",
      "20            180               16          83         27          75   \n",
      "21             38                2          20          2           8   \n",
      "22            214               27         130         33          96   \n",
      "23            239               19         157         48         104   \n",
      "24            121               10          79         19          59   \n",
      "25            159               15         101         19          75   \n",
      "26             67               13          32         10          24   \n",
      "27            180               30          95         33          74   \n",
      "28            157               20          94         24          75   \n",
      "29            117               11          63         21          70   \n",
      "...           ...              ...         ...        ...         ...   \n",
      "1753          248               31         152         32         117   \n",
      "1754          172               18          91         29          62   \n",
      "1755          244               32         147         45         116   \n",
      "1756          162               22          88         19          77   \n",
      "1757          154               11          83         27          57   \n",
      "1758          238               20         176         35          87   \n",
      "1759          166               11         105         30          79   \n",
      "1760           58                9          26         15          20   \n",
      "1761          184               19         113         24          85   \n",
      "1762          168               12          90         16          95   \n",
      "1763          125                3         120         31          83   \n",
      "1764          140               17          79         20          64   \n",
      "1765          190                6          99         28          71   \n",
      "1766          186               30          95         24          75   \n",
      "1767          233               17         163         34          95   \n",
      "1768          170               16          74         20          63   \n",
      "1769          184               12         133         34          73   \n",
      "1770          162               17          98         21          72   \n",
      "1771          167               24          92         23          85   \n",
      "1772          215               11         128         28         129   \n",
      "1773           70                8          35          3          19   \n",
      "1774          221               26         130         30          80   \n",
      "1775          220               19         139         30          94   \n",
      "1776          190               10         112         32          97   \n",
      "1777          168               18          97         19          63   \n",
      "1778          206               31         114         35         108   \n",
      "1779          109               13          53         13          46   \n",
      "1780          100                6          82         15          56   \n",
      "1781           14                0           2          3           2   \n",
      "1782          120               14          52         18          41   \n",
      "\n",
      "      adv_count  \n",
      "0            24  \n",
      "1            19  \n",
      "2            16  \n",
      "3            29  \n",
      "4            36  \n",
      "5            17  \n",
      "6            38  \n",
      "7            29  \n",
      "8            30  \n",
      "9            37  \n",
      "10           20  \n",
      "11           29  \n",
      "12            7  \n",
      "13           12  \n",
      "14           10  \n",
      "15           23  \n",
      "16           23  \n",
      "17           38  \n",
      "18            1  \n",
      "19            9  \n",
      "20           39  \n",
      "21            1  \n",
      "22           24  \n",
      "23           34  \n",
      "24           12  \n",
      "25           15  \n",
      "26            8  \n",
      "27           20  \n",
      "28           22  \n",
      "29           12  \n",
      "...         ...  \n",
      "1753         35  \n",
      "1754         22  \n",
      "1755         47  \n",
      "1756         26  \n",
      "1757         21  \n",
      "1758         16  \n",
      "1759         24  \n",
      "1760          1  \n",
      "1761         21  \n",
      "1762         37  \n",
      "1763         31  \n",
      "1764         29  \n",
      "1765         22  \n",
      "1766         28  \n",
      "1767         49  \n",
      "1768         24  \n",
      "1769         20  \n",
      "1770         22  \n",
      "1771         31  \n",
      "1772         39  \n",
      "1773         12  \n",
      "1774         35  \n",
      "1775         29  \n",
      "1776         35  \n",
      "1777         23  \n",
      "1778         39  \n",
      "1779         10  \n",
      "1780          8  \n",
      "1781          0  \n",
      "1782         19  \n",
      "\n",
      "[1783 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "features_set1 = extract_features(data[data['essay_set'] == 1])\n",
    "\n",
    "print(features_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>lemma_count</th>\n",
       "      <th>spell_err_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adv_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1538</td>\n",
       "      <td>350</td>\n",
       "      <td>16</td>\n",
       "      <td>4.237143</td>\n",
       "      <td>162</td>\n",
       "      <td>11</td>\n",
       "      <td>83</td>\n",
       "      <td>18</td>\n",
       "      <td>74</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1870</td>\n",
       "      <td>423</td>\n",
       "      <td>20</td>\n",
       "      <td>4.312057</td>\n",
       "      <td>185</td>\n",
       "      <td>25</td>\n",
       "      <td>107</td>\n",
       "      <td>19</td>\n",
       "      <td>85</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1263</td>\n",
       "      <td>283</td>\n",
       "      <td>14</td>\n",
       "      <td>4.342756</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>82</td>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2642</td>\n",
       "      <td>530</td>\n",
       "      <td>27</td>\n",
       "      <td>4.813208</td>\n",
       "      <td>236</td>\n",
       "      <td>34</td>\n",
       "      <td>178</td>\n",
       "      <td>42</td>\n",
       "      <td>97</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>473</td>\n",
       "      <td>30</td>\n",
       "      <td>4.334038</td>\n",
       "      <td>190</td>\n",
       "      <td>19</td>\n",
       "      <td>114</td>\n",
       "      <td>32</td>\n",
       "      <td>90</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_set                                              essay  \\\n",
       "0          1  Dear local newspaper, I think effects computer...   \n",
       "1          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  char_count  word_count  sent_count  avg_word_len  \\\n",
       "0            8.0        1538         350          16      4.237143   \n",
       "1            9.0        1870         423          20      4.312057   \n",
       "2            7.0        1263         283          14      4.342756   \n",
       "3           10.0        2642         530          27      4.813208   \n",
       "4            8.0        2105         473          30      4.334038   \n",
       "\n",
       "   lemma_count  spell_err_count  noun_count  adj_count  verb_count  adv_count  \n",
       "0          162               11          83         18          74         24  \n",
       "1          185               25         107         19          85         19  \n",
       "2          145                5          82         20          52         16  \n",
       "3          236               34         178         42          97         29  \n",
       "4          190               19         114         32          90         36  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 13)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data setup\n",
    "\n",
    "marks = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "#Number of examples\n",
    "m = features_set1.shape[0]\n",
    "#Features\n",
    "n = 10\n",
    "#Number of classes\n",
    "k = 15\n",
    "\n",
    "X = np.ones((m,n + 1))\n",
    "y = np.array((m,1))\n",
    "X[:,1] = features_set1['char_count'].values\n",
    "X[:,2] = features_set1['word_count'].values\n",
    "X[:,3] = features_set1['sent_count'].values\n",
    "X[:,4] = features_set1['avg_word_len'].values\n",
    "X[:,5] = features_set1['lemma_count'].values\n",
    "X[:,6] = features_set1['spell_err_count'].values\n",
    "X[:,7] = features_set1['noun_count'].values\n",
    "X[:,8] = features_set1['adj_count'].values\n",
    "X[:,9] = features_set1['verb_count'].values\n",
    "X[:,10] = features_set1['adv_count'].values\n",
    "\n",
    "#Labels\n",
    "y = features_set1['domain1_score'].values\n",
    "\n",
    "#Mean normalization\n",
    "for j in range(n):\n",
    "    X[:, j] = (X[:, j] - X[:,j].mean())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "#Regularized cost function\n",
    "def regCostFunction(theta, X, y, _lambda = 0.1):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    reg = (_lambda/(2 * m)) * np.sum(theta**2)\n",
    "\n",
    "    return (1 / m) * (-y.T.dot(np.log(h)) - (1 - y).T.dot(np.log(1 - h))) + reg\n",
    "\n",
    "#Regularized gradient function\n",
    "def regGradient(theta, X, y, _lambda = 0.1):\n",
    "    m, n = X.shape\n",
    "    theta = theta.reshape((n, 1))\n",
    "    y = y.reshape((m, 1))\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    reg = _lambda * theta /m\n",
    "\n",
    "    return ((1 / m) * X.T.dot(h - y)) + reg\n",
    "\n",
    "#Optimal theta \n",
    "def logisticRegression(X, y, theta):\n",
    "    result = op.minimize(fun = regCostFunction, x0 = theta, args = (X, y),\n",
    "                         method = 'TNC', jac = regGradient)\n",
    "    \n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "all_theta = np.zeros((k, n + 1))\n",
    "\n",
    "#One vs all\n",
    "i = 0\n",
    "for x in marks:\n",
    "    #set the labels in 0 and 1\n",
    "    tmp_y = np.array(y_train == x, dtype = int)\n",
    "    optTheta = logisticRegression(X_train, tmp_y, np.zeros((n + 1,1)))\n",
    "    all_theta[i] = optTheta\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy  48.4593837535014 %\n"
     ]
    }
   ],
   "source": [
    "#Predictions\n",
    "P = sigmoid(X_test.dot(all_theta.T)) #probability for each mark\n",
    "p = [marks[np.argmax(P[i, :])] for i in range(X_test.shape[0])]\n",
    "\n",
    "print(\"Test Accuracy \", accuracy_score(y_test, p) * 100 , '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
